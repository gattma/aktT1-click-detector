{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def apply_region_filter(before, after):\n",
    "    before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)\n",
    "    after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)\n",
    "    (score, diff) = ssim(before_gray, after_gray, full=True)\n",
    "    diff = (diff * 255).astype(\"uint8\")\n",
    "    thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "\n",
    "    before_region = np.zeros(before.shape, np.uint8)\n",
    "    after_region = np.zeros(after.shape, np.uint8)\n",
    "\n",
    "    for c in contours:\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > 40:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            before_region[y:y + h, x:x + w] = before[y:y + h, x:x + w]\n",
    "            after_region[y:y + h, x:x + w] = after[y:y + h, x:x + w]\n",
    "\n",
    "    return before_region, after_region"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def create_conv_layers(input_img, input_shape):\n",
    "    model = layers.Conv2D(32, (3, 3), padding='same', input_shape=input_shape)(input_img)\n",
    "    model = layers.LeakyReLU(alpha=0.1)(model)\n",
    "    model = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(model)\n",
    "    model = layers.Dropout(0.25)(model)\n",
    "\n",
    "    model = layers.Conv2D(64, (3, 3), padding='same')(model)\n",
    "    model = layers.LeakyReLU(alpha=0.1)(model)\n",
    "    model = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(model)\n",
    "    model = layers.Dropout(0.25)(model)\n",
    "\n",
    "    model = layers.Conv2D(96, (3, 3), padding='same')(model)\n",
    "    model = layers.LeakyReLU(alpha=0.1)(model)\n",
    "    model = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(model)\n",
    "    model = layers.Dropout(0.25)(model)\n",
    "\n",
    "    model = layers.Conv2D(128, (3, 3), padding='same')(model)\n",
    "    model = layers.LeakyReLU(alpha=0.1)(model)\n",
    "    model = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(model)\n",
    "    model = layers.Dropout(0.25)(model)\n",
    "\n",
    "    model = layers.Conv2D(256, (3, 3), padding='same')(model)\n",
    "    model = layers.LeakyReLU(alpha=0.1)(model)\n",
    "    model = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(model)\n",
    "    model = layers.Dropout(0.4)(model)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "as_gray = False  # use rgb or gray images?\n",
    "in_channels = 3  # gray = 1, rgb = 4\n",
    "if as_gray:\n",
    "    in_channels = 1\n",
    "\n",
    "img_height, img_width = 584, 480  # original\n",
    "\n",
    "num_classes = 2  # click or no click\n",
    "batch_size = 8\n",
    "epochs = 2\n",
    "\n",
    "train_dir_1 = \"dataset/train/before\"\n",
    "train_dir_2 = \"dataset/train/after\"\n",
    "\n",
    "test_dir_1 = \"dataset/test/before\"\n",
    "test_dir_2 = \"dataset/test/after\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator, dir1, dir2, batch_size, img_height, img_width):\n",
    "    genX1 = generator.flow_from_directory(dir1,\n",
    "                                          target_size=(img_height, img_width),\n",
    "                                          class_mode='categorical',\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          seed=666)\n",
    "\n",
    "    genX2 = generator.flow_from_directory(dir2,\n",
    "                                          target_size=(img_height, img_width),\n",
    "                                          class_mode='categorical',\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          seed=666)\n",
    "\n",
    "    while True:\n",
    "        before_imgs = []\n",
    "        after_imgs = []\n",
    "\n",
    "        X1i = genX1.next()\n",
    "        X2i = genX2.next()\n",
    "\n",
    "        for idx in range(len(X1i[0])):\n",
    "            before = np.reshape(X1i[0][idx], (584, 480, 3))\n",
    "            after = np.reshape(X2i[0][idx], (584, 480, 3))\n",
    "            before, after = apply_region_filter(before.copy(), after.copy())\n",
    "\n",
    "            before = np.asarray(before)\n",
    "            before = before.astype('float32')\n",
    "            before /= 255.0\n",
    "\n",
    "            after = np.asarray(before)\n",
    "            after = after.astype('float32')\n",
    "            after /= 255.0\n",
    "\n",
    "            before_imgs.append(before)\n",
    "            after_imgs.append(after)\n",
    "\n",
    "        before = np.reshape(before_imgs, (len(X1i[0]), 584, 480, 3))\n",
    "        after = np.reshape(after_imgs, (len(X1i[0]), 584, 480, 3))\n",
    "        yield [before, after], X2i[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "input_shape = (img_height, img_width, in_channels)\n",
    "\n",
    "before_input = layers.Input(shape=input_shape)\n",
    "before_model = create_conv_layers(before_input, input_shape)\n",
    "\n",
    "after_input = layers.Input(shape=input_shape)\n",
    "after_model = create_conv_layers(after_input, input_shape)\n",
    "\n",
    "conv = layers.concatenate([before_model, after_model])\n",
    "conv = layers.Flatten()(conv)\n",
    "\n",
    "dense = layers.Dense(512)(conv)\n",
    "dense = layers.LeakyReLU(alpha=0.1)(dense)\n",
    "dense = layers.Dropout(0.25)(dense)\n",
    "\n",
    "dense = layers.Dense(256)(dense)\n",
    "dense = layers.LeakyReLU(alpha=0.1)(dense)\n",
    "dense = layers.Dropout(0.25)(dense)\n",
    "\n",
    "dense = layers.Dense(128)(dense)\n",
    "dense = layers.LeakyReLU(alpha=0.1)(dense)\n",
    "dense = layers.Dropout(0.4)(dense)\n",
    "\n",
    "output = layers.Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[before_input, after_input], outputs=[output])\n",
    "\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy', 'accuracy'])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('ClickDetector{epoch:02d}.h5', period=1, save_weights_only=True)\n",
    "callbacks = [model_checkpoint]\n",
    "\n",
    "generator = ImageDataGenerator(rescale=1,  # 1./255,\n",
    "                               rotation_range=0,\n",
    "                               width_shift_range=0,\n",
    "                               height_shift_range=0,\n",
    "                               shear_range=0,\n",
    "                               zoom_range=0,\n",
    "                               horizontal_flip=False,\n",
    "                               vertical_flip=False\n",
    "                               )\n",
    "\n",
    "inputgenerator = generate_generator_multiple(generator=generator,\n",
    "                                             dir1=train_dir_1,\n",
    "                                             dir2=train_dir_2,\n",
    "                                             batch_size=batch_size,\n",
    "                                             img_height=img_height,\n",
    "                                             img_width=img_width)\n",
    "\n",
    "testgenerator = generate_generator_multiple(generator,\n",
    "                                            dir1=test_dir_1,\n",
    "                                            dir2=test_dir_2,\n",
    "                                            batch_size=batch_size,\n",
    "                                            img_height=img_height,\n",
    "                                            img_width=img_width)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "51/50 - 32s - loss: 2.6879 - categorical_accuracy: 0.8272 - accuracy: 0.8272 - val_loss: 1.0008 - val_categorical_accuracy: 0.9412 - val_accuracy: 0.9412\n",
      "Epoch 2/2\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "history = model.fit_generator(inputgenerator,\n",
    "                              steps_per_epoch=404 / batch_size,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=testgenerator,\n",
    "                              validation_steps=102 / batch_size,\n",
    "                              # use_multiprocessing=True,\n",
    "                              shuffle=False,\n",
    "                              verbose=2,\n",
    "                              callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}